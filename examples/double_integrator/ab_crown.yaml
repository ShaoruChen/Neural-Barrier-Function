# This is an example configuration file that contains most useful parameter settings.
general:
  device: cpu
  seed: 0
  record_bounds: True
model:
  name: resnet2b  # This model is defined in model_defs.py. Add your own model definitions there.
  path: models/cifar10_resnet/resnet2b.pth  # Path to PyTorch checkpoint.
  input_shape: -1 2
data:
  dataset: CIFAR  # Dataset name. This is just the standard CIFAR-10 test set defined in the "load_verification_dataset()" function in utils.py
  mean: [ 0.4914, 0.4822, 0.4465 ]  # Mean for normalization.
  std: [ 0.2471, 0.2435, 0.2616 ]  # Std for normalization.
  start: 0  # First example to verify in dataset.
  end: 100
attack: # Currently attack is only implemented for Linf norm.
  pgd_steps: 100  # Increase for a stronger attack. A PGD attack will be used before verification to filter on non-robust data examples.
  pgd_restarts: 30  # Increase for a stronger attack.
solver:
  bound_prop_method: 'alpha-crown'
  batch_size: 2048  # Number of subdomains to compute in parallel in bound solver. Decrease if you run out of memory.
  alpha-crown:
    iteration: 100   # Number of iterations for alpha-CROWN optimization. Alpha-CROWN is used to compute all intermediate layer bounds before branch and bound starts.
    lr_alpha: 0.1    # Learning rate for alpha in alpha-CROWN. The default (0.1) is typically ok.
  beta-crown:
#    all_node_split_LP: True # error occurs when used
    lr_alpha: 0.01  # Learning rate for optimizing the alpha parameters, the default (0.01) is typically ok, but you can try to tune this parameter to get better lower bound.
    lr_beta: 0.05  # Learning rate for optimizing the beta parameters, the default (0.05) is typically ok, but you can try to tune this parameter to get better lower bound.
    iteration: 20  # Number of iterations for beta-CROWN optimization. 20 is often sufficient, 50 or 100 can also be used.
bab:
  get_upper_bound: true
  pruning_in_iteration: false
#  attack:
#    enabled: True
#    beam_candidates: 128
#    beam_depth: 6
#    max_dive_fix_ratio: 0.5
#    min_local_free_ratio: 0.5
#    mip_timeout: 360
#    mip_start_iteration: 2
#    refined_mip_attacker: true
  timeout: 120 # Timeout threshold for branch and bound. Increase for verifying more points.
  branching: # Parameters for branching heuristics.
    reduceop: min  # Reduction function for the branching heuristic scores, min or max. Using max can be better on some models.
    method: kfsb  # babsr is fast but less accurate; fsb is slow but most accurate; kfsb is usually a balance.
    candidates: 3  # Number of candidates to consider in fsb and kfsb. More leads to slower but better branching. 3 is typically good enough.
# options for training the barrier function
alg_options:
  train_method: fine-tuning # use 'verification-only' or 'fine-tuning'
  verification_method: mip # use 'bab' or 'mip'
  dynamics_fcn:
    train_nn_dynamics: False
  barrier_fcn:
    barrier_output_dim: 5
    dataset:
      train_data_set_path: data/B_train_dataset.p
      collect_samples: False
      # number of samples to collect in order to train the barrier function
      num_samples_x0: 5000
      num_samples_xu: 5000
      num_samples_x: 10000
    train_options: # training parameters of the barrier function in one round
      num_epochs: 50
      l1_lambda: 0.0
      early_stopping_tol: 0.000000001
      update_A_freq: 1 # update the A matrix every x epochs
      samples_pool_size: 100000 # maximum number of training samples for each condition
      B_batch_size: 50 # batch size in training the barrier function
      num_iter: 200 # number of the learn-verify iteration
      scaling_factor: 0.5 # scaling factor in the shrink-and-perturb strategy
      noise_weight: 1.0 # the noise weight in the shrink-and-perturb strategy
      train_timeout: 7200
  ce_sampling: # options that generate additional samples in the neighborhood of a counterexample through gradient descent
    num_ce_samples: 50 # number of additional samples generated around one counterexample
    opt_iter: 100 # number of gradient descent steps
    radius: 0.1 # radius of ell_inf ball around each counterexample
    num_ce_samples_accpm: 10
  ACCPM:
    max_iter: 30
    cvxpy_solver: ECOS
    num_ce_thresh: 5 # max. number of counterexamples added to the sample set in each iteration
  bab:
    bab_yaml_path: 'ab_crown.yaml'
    adv_sample_filter_radius_x0: 0.1
    adv_sample_filter_radius_xu: 0.1
    adv_sample_filter_radius_x: 0.3
    adv_samples_pool_size: 50 # pool size in generating upper bounds in bab
    get_upper_bound_samples: 2048 # additional samples to refine the upper bound
  mip: # store mip solver options
    time_limit: 0.0 # 0.0 means no time limit
    MIPFocus: 0 # range: [0,1,2,3]. See guorbi webpage https://www.gurobi.com/documentation/current/refman/mipfocus.html for explanation



